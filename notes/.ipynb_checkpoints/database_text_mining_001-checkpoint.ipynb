{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data and databases\n",
    "###Text mining I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###computers not so smart\n",
    "###computer not so good with non numerical things\n",
    "###**Choose** to make tractable--**Choose** to lose to learn something couldn't otherwise! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vectoring text, or: the world is too hard to understand easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##let's get fairly clean text data\n",
    "##from the Capitol Words Project\n",
    "\n",
    "The capitol words project http://capitolwords.org/api/1/. \n",
    "\n",
    "This is one of the APIs for the Sunlight Foundation. http://sunlightfoundation.com/api/\n",
    "\n",
    "    payload={\"phrase\":\"national security agency\", \"page\":0, \"apikey\":\"YER KEY HERE\"}\n",
    "    r=requests.get(\"http://capitolwords.org/api/1/text.json\", params=payload)\n",
    "\n",
    "You'll need to loop through all the pages--only 50 results at a time.\n",
    "\n",
    "So as not to overload their servers, let's just all download from my website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-41c290f897d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"phrase\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"national security agency\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"page\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apikey\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"8b8e8ba86d71474dabc31236dd47cecd\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://capitolwords.org/api/1/text.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "payload={\"phrase\":\"national security agency\", \"page\":0, \"apikey\":\"8b8e8ba86d71474dabc31236dd47cecd\"}\n",
    "r=requests.get(\"http://capitolwords.org/api/1/text.json\", params=payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TIME to *vectorize*\n",
    "\n",
    "Convert this bunch of text into a format more tractable to computer. \n",
    "\n",
    "###a big violence upon the text\n",
    "\n",
    "Let's start with someting a bit easier and then come back to our documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_texts=[\"The cat sat on the mat\", \n",
    "           \"The cat saw the other cat on Sat while she sat\", \n",
    "           \"Excellent Smithers! Use the saw on the cat in the magic trick with the dog\",\n",
    "            \"Excellent magic saw Smithers on the dog\",\n",
    "           \"Excellent magic sat Smithers on the dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terms < sentences < documents < corpus\n",
    "\n",
    "- Shakespeare\n",
    "- enron email corpus\n",
    "- large number of tweets\n",
    "- congress critter speak\n",
    "\n",
    "*really matters* how you define your corpus: if you want to bring out what's odd about beltway cant, then likely need to have a corpus with a sizeable chunk of beltway talk, and a sizeable chunk of non-beltway talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Term frequency [TF]\n",
    "The \"term frequency\" measures how often a given term occurs in a given document.\n",
    "\n",
    "We count up how many times each term appears in a document, then divide it by the number of terms in the document.\n",
    "\n",
    "For a word $t$ that appears $i_w$ times in a document $D$ with a number of words $n_D$, the term frequency is\n",
    "\n",
    "$tf(t,D)=\\frac{i_w}{n_D}$\n",
    "\n",
    "Can you think of a problem with this as a measure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse document frequency\n",
    "Often we'll divide the term frequency using some measure of how unusual each word is across all the documents in question. \n",
    "\n",
    "If we were reading general political news stories for the last few years, \"Obama\" appears *a lot* in each document and in lots of documents. \"Butte\" appears, say, a lot in one document but not at all in the rest.\n",
    "\n",
    "We want something that will help us to see that \"Butte\" is really significant for capturing something distinctive about that document, whereas \"Obama\" wouldn't be. \n",
    "\n",
    "So we compute the *inverse document frequency*. [IDF]\n",
    "\n",
    "You divide the total number of documents ($N$)\n",
    "by one plus the number of documents containing each word t ($n_w$)\n",
    "\n",
    "$\\frac{N}{1+n_w}$\n",
    "\n",
    "Think about what is this does:\n",
    "for a word that appears in *every* document will thus be \n",
    "\n",
    "\n",
    "$\\frac{N}{1+n_w}=\\frac{N}{1+N}\\approx 1$ \n",
    "\n",
    "Where a word appears that in only *one* document will have a much bigger scaling factor:\n",
    "\n",
    "\n",
    "$\\frac{N}{1+n_w}=\\frac{N}{2}$.\n",
    "\n",
    "Typically, we take the log of this to get:\n",
    "\n",
    "$idf(t,D)=\\log(\\frac{N}{1+n_w})$.\n",
    "\n",
    "So we'll computer what's called tf-idf in the biz by multiplying the frequency and the inverse document frequency:\n",
    "\n",
    "$tfidf=tf\\times idf$\n",
    "\n",
    "\n",
    "As so often, this is not a neutral choice: \n",
    "\n",
    "- if we pick $tf$ by itself, we want the most frequent words normalized by length in each document. \n",
    "\n",
    "- if we pick $tfidf$, then we are saying we want to work with the most frequent words that are also *unusual* across our particular set of documents.\n",
    "\n",
    "If we use tfidf on a set of documents about the CIA from 2000-2010, \"intelligence\" would be in most of them, we'd guess, and so the measure would down-play them in favor of what makes each document more distinctive *within* the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#What we need to clean up text\n",
    "\n",
    "\n",
    "##tokenization\n",
    "making `.split` much better\n",
    "\n",
    "Examples??\n",
    "\n",
    "##stemming:\n",
    "- converting inflected forms into some normalized forms\n",
    "    - e.g. \"chefs\" --> \"chef\"\n",
    "    - \"goes\" --> \"go\"\n",
    "    - \"children\" --> \"child\"\n",
    "\n",
    "##stopwords\n",
    "they are the words you don't want to be included:\n",
    "\"from\" \"to\" \"a\" \"they\" \"she\" \"he\"\n",
    "\n",
    "##others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here's our help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#How to process text\n",
    "\n",
    "###Python Libraries\n",
    "\n",
    "Python has an embarrasment of riches when it comes to working with texts. Some libraries are higher level with simpler, well thought out defaults, namely `pattern` and `TextBlob`. Most general, of long development, and foundational is the Natural Language Tool Kit--NLTK. The ideas we'll learn to today are key--they have slightly different instantiations in the different tools. Not everything is yet in Python 3, so right now 2.7 a safer bet!!\n",
    "\n",
    "###nltk : grandparent of text analysis packages, cross-platform, complex\n",
    "\n",
    "+ crucial for moving beyond bag of words: tagging & other grammatical analysis\n",
    "+ good for stemmers, lemmatizers, etc.\n",
    "+ in conda\n",
    "\n",
    "###pattern : higher level and easier to use the nltk but Python 2.7 only.\n",
    "    http://www.clips.ua.ac.be/pages/pattern-vector\n",
    "\n",
    "###textblob : even higher level range of natural language processing\n",
    "\n",
    "###scikit learn (sklearn): toolkit for scientists, faster, better (use for processing/memory intensive stuff) \n",
    "\n",
    "+ in conda\n",
    "\n",
    "###gensim : powerful collection of scalable text analysis tools; use next time\n",
    "\n",
    "+ in conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll play with the cool scientists and use the powerful and fast scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_texts=[\"The cat sat on the mat\", \n",
    "           \"The cat saw the other cat on Sat while she sat\", \n",
    "           \"Excellent Smithers! Use the saw on the cat in the magic trick with the dog\",\n",
    "            \"Excellent magic saw Smithers on the dog\",\n",
    "           \"Excellent magic sat Smithers on the dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(stop_words='english', use_idf=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_term_matrix=vectorizer.fit_transform(our_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now that our texts are _happy vectors_ we can throw wide variety of mining algorithms at our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Similarity and dissimilarity\n",
    "\n",
    "We reduced our text to a vector of term-weights. What can we do once we've committed this violence on the text?\n",
    "\n",
    "We can measure *distance* and *similarity*\n",
    "\n",
    "I know. Crazy talk.\n",
    "\n",
    "Right now our text is just a series of numbers, indexed to words. We can treat it like any other set of words.\n",
    "\n",
    "And the key way to distinguish two vectors is by measuring their distance or computing their similiarity (`1-distance`).\n",
    "\n",
    "You already know how, though you may have buried it along with memories of high school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Many distance metrics to choose from\n",
    "## key one in textual analysis:\n",
    "### cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If $\\mathbf{a}$ and $\\mathbf{b}$ are vectors, then\n",
    "\n",
    "$\\mathbf{a}\\cdot\\mathbf{b}=\\left\\|\\mathbf{a}\\right\\|\\left\\|\\mathbf{b}\\right\\|\\cos\\theta$\n",
    "\n",
    "Or\n",
    "\n",
    "$\\text{similarity} = \\cos(\\theta) = {A \\cdot B \\over \\|A\\| \\|B\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i \\times B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{(A_i)^2}} \\times \\sqrt{\\sum\\limits_{i=1}^{n}{(B_i)^2}} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#easy to program, but let's use a robust version\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Back to some real data: NSA mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(min_df=0.7, stop_words='english', use_idf=True)\n",
    "document_term_matrix=vectorizer.fit_transform(#insert name for speeches here##)\n",
    "vocab=vectorizer.get_feature_names()\n",
    "\n",
    "## Key thing here:min_df=.7!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Let's try some plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "#we can make a heatmap with no problems within mathplotlib\n",
    "#pass plt.pcolor our similiarity matrix\n",
    "plt.pcolor(similarity, norm=None, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we have too many documents for that to be very useful; so\n",
    "plt.pcolor(similarity[100:110, 100:110], norm=None, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#supervised vs. unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##first example of unsupervised learning\n",
    "###hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "dtm=document_term_matrix\n",
    "dtm_trans=dtm.T\n",
    "dist=1-cosine_similarity(dtm_trans)\n",
    "linkage_matrix=ward(dist)\n",
    "\n",
    "#plot dendogram\n",
    "\n",
    "f=plt.figure(figsize=(9,9))\n",
    "R=dendrogram(linkage_matrix, orientation=\"right\", labels=vocab)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##is this significant? Are there interesting patterns to seek out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##here's what we're up to:\n",
    "\n",
    "Exploratory data analysis (EDA) seeks to reveal structure, or simple descriptions, in data. We look at numbers and graphs and try to find patterns. \n",
    "\n",
    "-. . . we can view the techniques of EDA as a ritual designed to reveal patters in a data set. Thus, we may believe that naturally occurring data sets contain structure, that EDA is a useful vehicle for revealing the structure. . . . If we make no attempt to check whether the structure could have arisen by chance, and tend to accept the findings as gospel, then the ritual comes close to magical thinking. ... a controlled form of magical thinking--in the guise of 'working hypothesis'--is a basic ingredient of scientific progress. \n",
    "- Persi Diaconis, \"Theories of Data Analysis: From Magical Thinking Through Classical statistics\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##need to elicit patterns and avoid bad magical thinking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
